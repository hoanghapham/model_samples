{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d9de97",
   "metadata": {},
   "source": [
    "# Power Identification with NN and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06e26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from typing import Self\n",
    "import csv\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model_samples.torch.nn import NNClassifier\n",
    "from model_samples.torch.rnn import PositionalEncoder, RNNClassifier\n",
    "from model_samples.utils import TrainConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c15d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset():\n",
    "    \"\"\"Class to hold raw data load directly from the tsv files.\n",
    "    \"\"\"\n",
    "    def __init__(self, ids: list[str], speakers: list[str], texts: list[str], labels: list[int]) -> None:\n",
    "        assert len(ids) == len(speakers) == len(texts) == len(labels), \"All arrays must have the same length\"\n",
    "        self.ids = ids\n",
    "        self.speakers = speakers\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def subset(self, index_list: list[int]):\n",
    "\n",
    "        data = RawDataset(\n",
    "            [self.ids[idx] for idx in index_list],\n",
    "            [self.speakers[idx] for idx in index_list],\n",
    "            [self.texts[idx] for idx in index_list],\n",
    "            [self.labels[idx] for idx in index_list],\n",
    "        )\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return (self.ids[index], self.speakers[index], self.texts[index], self.labels[index])\n",
    "\n",
    "    def __add__(self, other: Self):\n",
    "        return RawDataset(\n",
    "            self.ids + other.ids,\n",
    "            self.speakers + other.speakers,\n",
    "            self.texts + other.texts,\n",
    "            self.labels + other.labels\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in zip(self.ids, self.speakers, self.texts, self.labels):\n",
    "            yield data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def load_data(file_path, encoding: str = 'utf-8', text_head: str = 'text') -> RawDataset:\n",
    "    \"\"\"Load one file and return \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    return RawDataset(data[\"id\"], data[\"speaker\"], data[\"text\"], data[\"label\"])\n",
    "\n",
    "\n",
    "class EncodedDataset(Dataset):\n",
    "    \"\"\"Custom Dataset object to hold parliament debate data. Each item in the dataset\n",
    "    is a tuple of (input tensor, label)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            inputs: torch.Tensor, \n",
    "            labels: torch.Tensor, \n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        assert len(inputs) == len(labels), \"Inputs and labels have different length\"\n",
    "        self.data_ = list(zip(inputs, labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for data in self.data_:\n",
    "            yield data\n",
    "\n",
    "def encode_torch_data(data: RawDataset, encoder: PositionalEncoder | TfidfVectorizer):\n",
    "    \"\"\"Convenience function to create the encoded dataset compatible with torch models\"\"\"\n",
    "    # Encode text\n",
    "    enc_texts_csr = encoder.transform(data.texts)  \n",
    "    \n",
    "    if isinstance(enc_texts_csr, csr_matrix):\n",
    "        inputs = torch.from_numpy(enc_texts_csr.todense()).float()\n",
    "    else:\n",
    "        inputs = enc_texts_csr.to_dense()\n",
    "    \n",
    "    # Convert labels to tensor\n",
    "    labels = torch.tensor(data.labels)\n",
    "\n",
    "    return EncodedDataset(inputs, labels)\n",
    "\n",
    "# Helper functions\n",
    "def get_average_metrics(result_list: list[dict]) -> dict:\n",
    "    accuracy = np.mean([[result['accuracy'] for result in result_list]])\n",
    "    precision = np.mean([[result['precision'] for result in result_list]])\n",
    "    recall = np.mean([[result['recall'] for result in result_list]])\n",
    "    f1 = np.mean([[result['f1'] for result in result_list]])\n",
    "    auc = np.mean([[result['auc'] for result in result_list]])\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"auc\": auc}\n",
    "\n",
    "\n",
    "def evaluate(y_test: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray) -> dict:\n",
    "    \"\"\"Conveninece function to evaluate predction of models.\n",
    "\n",
    "    The function returns a dictionary of metrics:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1\n",
    "    - AUC\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : np.ndarray\n",
    "        Labels of the test set\n",
    "    y_pred : np.ndarray\n",
    "        Prediction produced by the model\n",
    "    y_prob : np.ndarray\n",
    "        Probability array produced by the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "\n",
    "    true_pos = sum([pred == y == 1 for pred, y in zip(y_pred, y_test)])\n",
    "    true_neg = sum([pred == y == 0 for pred, y in zip(y_pred, y_test)])\n",
    "    false_pos = sum([(pred == 1) * (y == 0) for pred, y in zip(y_pred, y_test)])\n",
    "    false_neg = sum([(pred == 0) * (y == 1) for pred, y in zip(y_pred, y_test)])\n",
    "    total = len(y_test)\n",
    "\n",
    "    accuracy = (true_pos + true_neg) / total\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2 * true_pos / (2 * true_pos + false_pos + false_neg)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    # if isinstance(accuracy, torch.Tensor):\n",
    "    #     result = {\n",
    "    #         \"accuracy\": accuracy.item(),\n",
    "    #         \"precision\": precision.item(),\n",
    "    #         \"recall\": recall.item(),\n",
    "    #         \"f1\": f1.item(),\n",
    "    #         \"auc\": auc.item(),\n",
    "    #     }    \n",
    "    # else:\n",
    "\n",
    "    result = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb50f1",
   "metadata": {},
   "source": [
    "# Classification with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd69087",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"./data/power-gb-train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46fe0d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV fold 1\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     35\u001b[39m t0 = datetime.datetime.now()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m time_elapsed = (datetime.datetime.now() - t0).total_seconds()\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m train time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_elapsed\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/model_samples/src/model_samples/torch/nn.py:137\u001b[39m, in \u001b[36mNNClassifier.fit\u001b[39m\u001b[34m(self, train_dataloader, train_config, disable_progress_bar)\u001b[39m\n\u001b[32m    135\u001b[39m loss = loss_function(logits, y_train)\n\u001b[32m    136\u001b[39m loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Evaluate on train set \u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/model_samples/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/model_samples/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/model_samples/.venv/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/model_samples/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/model_samples/.venv/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/model_samples/.venv/lib/python3.11/site-packages/torch/optim/adam.py:525\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    523\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m         denom = \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "NFOLDS = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "result_list = []\n",
    "kfold = KFold(n_splits=NFOLDS, shuffle=False, random_state=None)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(data), start=1):\n",
    "    print(f\"CV fold {fold_idx}\")\n",
    "\n",
    "    encoder = TfidfVectorizer(max_features=50000, analyzer=\"char\", ngram_range=(3,5), use_idf=True, sublinear_tf=True)\n",
    "    encoder.fit(data.subset(train_idx).texts)\n",
    "\n",
    "    # Encode data\n",
    "    train_data = encode_torch_data(data.subset(train_idx), encoder)\n",
    "    test_data = encode_torch_data(data.subset(test_idx), encoder)\n",
    "\n",
    "    # Init model\n",
    "    train_config = TrainConfig(\n",
    "        num_epochs      = 10,\n",
    "        early_stop      = False,\n",
    "        violation_limit = 5\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "    model = NNClassifier(\n",
    "        input_size=len(encoder.vocabulary_),\n",
    "        hidden_size=64,\n",
    "        n_linear_layers=3,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    t0 = datetime.datetime.now()\n",
    "    model.fit(dataloader, train_config, disable_progress_bar=True)\n",
    "    time_elapsed = (datetime.datetime.now() - t0).total_seconds()\n",
    "    \n",
    "    print(f\"Fold {fold_idx} train time: {time_elapsed / 60:.4} minutes\")\n",
    "\n",
    "\n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        X_test_nn = torch.stack([test[0] for test in test_data]).cpu()\n",
    "        y_test_nn = torch.stack([test[1] for test in test_data]).cpu()\n",
    "        y_pred_nn = model.predict(X_test_nn)\n",
    "        logits_nn = model.forward(X_test_nn)\n",
    "\n",
    "    result = evaluate(y_test_nn.cpu(), y_pred_nn.cpu(), logits_nn.cpu())\n",
    "    result_list.append({\"fold\": str(fold_idx), **result})\n",
    "\n",
    "\n",
    "avg_nn_results = get_average_metrics(result_list)\n",
    "print([f\"{key}: {value:.3f}\" for key, value in avg_nn_results.items()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6069a",
   "metadata": {},
   "source": [
    "# Classification with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e565c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NFOLDS = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "result_list = []\n",
    "kfold = KFold(n_splits=NFOLDS, shuffle=False)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(data), start=1):\n",
    "    print(f\"CV fold {fold_idx}\")\n",
    "\n",
    "    chars_encoder = TfidfVectorizer(max_features=50000, analyzer=\"char\", ngram_range=(3,5), use_idf=True, sublinear_tf=True)\n",
    "    encoder = PositionalEncoder(tokenizer=chars_encoder.build_tokenizer())\n",
    "    encoder.fit(data.subset(train_idx).texts)\n",
    "\n",
    "    train_dataloader = DataLoader(data.subset(train_idx), batch_size=128, shuffle=True)\n",
    "    test_dataloader = DataLoader(data.subset(test_idx), batch_size=128, shuffle=False)\n",
    "\n",
    "    # Prepare baseline config\n",
    "    train_config = TrainConfig(\n",
    "        optimizer_params = {'lr': 0.01},\n",
    "        num_epochs       = 10,\n",
    "        early_stop       = False,\n",
    "        violation_limit  = 5\n",
    "    )\n",
    "\n",
    "    # Train baseline model\n",
    "    model = RNNClassifier(\n",
    "        rnn_network         = nn.LSTM,\n",
    "        word_embedding_dim  = 32,\n",
    "        hidden_dim          = 64,\n",
    "        bidirectional       = False,\n",
    "        dropout             = 0,\n",
    "        encoder             = encoder,\n",
    "        device              = DEVICE\n",
    "    )\n",
    "\n",
    "    t0 = datetime.datetime.now()\n",
    "    model.fit(train_dataloader, train_config, no_progress_bar=True)\n",
    "\n",
    "    time_elapsed = (datetime.datetime.now() - t0).total_seconds()\n",
    "    print(f\"Fold {fold_idx} train time: {time_elapsed / 60:.4} minutes\")\n",
    "\n",
    "\n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        model.device = \"cpu\"\n",
    "        model.cpu()\n",
    "\n",
    "        pred_lst = []\n",
    "        probs_lst = []\n",
    "\n",
    "        for _, _, raw_inputs, raw_targets in test_dataloader:\n",
    "            batch_encoder = PositionalEncoder(vocabulary=encoder.vocabulary)\n",
    "            test_inputs = batch_encoder.fit_transform(raw_inputs).cpu()\n",
    "            # test_targets = torch.as_tensor(raw_targets, dtype=torch.float).cpu()\n",
    "            \n",
    "            pred_lst.append(model.predict(test_inputs))\n",
    "            probs_lst.append(model._sigmoid(model.forward(test_inputs)).squeeze())\n",
    "\n",
    "    pred = torch.cat(pred_lst).long().numpy()\n",
    "    probs = torch.concat(probs_lst).numpy()\n",
    "\n",
    "    result = evaluate(data.subset(test_idx).labels, pred, probs)\n",
    "    result_list.append({\"fold\": str(fold_idx), **result})\n",
    "\n",
    "\n",
    "avg_rnn_results = get_average_metrics(result_list)\n",
    "print([f\"{key}: {value:.3f}\" for key, value in avg_rnn_results.items()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
